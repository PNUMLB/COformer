# Enhancing Protein Expression in Humans through Codon Optimization with Transformer and Contrastive Learning

This project focuses on enhancing protein expression by employing advanced deep learning techniques, specifically transformer architectures combined with contrastive learning. Our approach, named **COformer**, is tailored for codon optimization in Homo sapiens and leverages pre-optimized sequences to further enhance the efficiency of protein production.

## Abstract

Codon optimization is a critical technique in molecular biology and genetic engineering aimed at improving protein expression by aligning gene codon usage with the host organismâ€™s translational machinery. Traditional methods, while effective, are resource-intensive and rely heavily on extensive biological knowledge. Recent advancements in computational biology have enabled the integration of deep learning into codon optimization, leading to significant improvements. Our methodology uses sequences pre-optimized by existing tools, providing a robust baseline for further enhancement. We have developed a deep learning framework that combines a transformer encoder to capture global sequence information with convolutional layers to refine local context. Additionally, we utilize a contrastive loss function to capture local codon dependencies. Empirical validation through in vitro experiments demonstrated that our deep learning-enhanced codon sequences significantly outperform those generated by traditional methods in terms of protein expression levels. These results underscore the transformative potential of deep learning in synthetic biology and establish a new benchmark for the field.

## Usage

The script `experiment.py` is designed to run experiments for codon optimization using the provided models and configurations. Below is a guide on how to use the script along with explanations for key arguments.

### Running the Script

To start the training process, use the following command:

```bash
python experiment.py --data_name <data_name> --name <model_name> --lambda_emb <lambda_value> --lambda_codon <lambda_value>
```
### Arguments
`--data_name`: The dataset to use for the experiment. This specifies the source of pre-optimized sequences and is critical for determining the baseline for further optimization.
- Options: all, genscript, thermofisher, novopro
- Default: all
- Example: Use genscript to select sequences optimized by the GenScript tool.

`--name`: The model architecture to use for the experiment. This defines the deep learning model that will be trained and evaluated.
- Options: coformer, bilstm, transformer
- Default: coformer
- Example: Use coformer for the hybrid model that combines convolutional and transformer layers.

`--lambda_emb`: The weight for the embedding loss in the contrastive learning framework. This parameter controls the influence of embedding similarity in the optimization process.
- Type: float
- Default: 0.0
- Example: Setting --lambda_emb 1.0 will apply the full weight of the embedding loss during training.

`--lambda_codon`: The weight for the codon loss in the contrastive learning framework. This parameter is crucial for optimizing the codon usage pattern based on the contrastive loss.
- Type: float
- Default: 0.0
- Example: Use --lambda_codon 1.0 to fully incorporate codon loss into the model training.

### Additional Arguments
For more advanced configurations, the script supports additional arguments:

`--epoch`: Number of training epochs. Default is 200.

`--batchsize`: Training batch size. Default is 512.

`--optimizer`: Optimizer choice. Options are adamw, adam, sgd. Default is adamw.

`--lr`: Learning rate. Default is 1e-4.

`--earlystop`: Number of epochs for early stopping. Default is 30.

`--wo_pad`: Flag to disable padding. Default is False.

`--verbose`: Flag for verbosity. Default is True.

`--file_name`: File name for saving the best model. Default is best_model.

`--window`: Window size for cross-entropy loss. Default is 64.

`--step_size`: Step size for cross-entropy loss. Default is 32.

`--seed`: Random seed for reproducibility. Default is 1.

`--device`: Device choice, where -1 is for CPU, and 0 or 1 are for GPU. Default is -1.








